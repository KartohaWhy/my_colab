{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KartohaWhy/my_colab/blob/main/Copy_%22LLM_RAG_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ò—Ç–æ–≥–æ–≤–æ–µ –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –∫—É—Ä—Å–∞\n",
        "\n",
        "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –í—ã –ø–æ—Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç–µ—Å—å –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á NLP, –∞ —Ç–∞–∫–∂–µ –ø–æ—Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å LLM –∏ —É–≥–ª—É–±–∏—Ç–µ—Å—å –≤ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª—è–º–∏.\n",
        "\n",
        "## –ß–∞—Å—Ç—å 1: —Ç—Ä–µ–Ω–∏—Ä—É–µ–º—Å—è –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö NLP-–∑–∞–¥–∞—á\n",
        "\n",
        "–ó–∞–≥—Ä—É–∑–∏–º –¥–∞—Ç–∞—Å–µ—Ç cnn-dailymail —Å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏."
      ],
      "metadata": {
        "id": "jWMJwXAz5y-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"gowrishankarp/newspaper-text-summarization-cnn-dailymail\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "jv9dksozukbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for file in os.listdir(path):\n",
        "    print(\"üìÑ\", file)"
      ],
      "metadata": {
        "id": "GePAgsIAuvJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_path = os.path.join(path, \"cnn_dailymail\")\n",
        "for file in os.listdir(inner_path):\n",
        "    print(\"üìÑ\", file)"
      ],
      "metadata": {
        "id": "jx9LSdF-vT-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(os.path.join(inner_path, \"train.csv\"))\n",
        "print(df.columns)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "7cM8EmJXvjyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ó–∞–¥–∞—á–∞ 1: —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "–≠—Ç—É –∑–∞–¥–∞—á—É –º—ã —Ä–µ—à–∏–º –∑–∞ –≤–∞—Å. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∞—è:\n",
        "\n",
        "- —Å–æ–∑–¥–∞–µ–º pipeline –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, —É–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "- –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ç–µ–∫—Å—Ç —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏)\n",
        "- –ø—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∫ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏, –ø—Ä–∏—á–µ–º —Ö–æ—Ç–∏–º —Å–∞–º–º–∞—Ä–∏ –¥–ª–∏–Ω—ã –æ—Ç 30 –¥–æ 130 —Ç–æ–∫–µ–Ω–æ–≤. –°–º–æ—Ç—Ä–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
      ],
      "metadata": {
        "id": "LTgFcPNqxBvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "lyc4dBWTwAlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['article'][0]"
      ],
      "metadata": {
        "id": "oiUCFoXjwrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarizer(df['article'][0], max_length=130, min_length=30, do_sample=False)\n",
        "summary[0]['summary_text']"
      ],
      "metadata": {
        "id": "qt_h2w8wwbgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ó–∞–¥–∞–Ω–∏–µ: –≤—ã–≤–µ–¥–∏—Ç–µ –≤ —Ü–∏–∫–ª–µ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏—Ö —Ç–µ–∫—Å—Ç –∏ —Å–∞–º–º–∞—Ä–∏."
      ],
      "metadata": {
        "id": "pGz_yoE6yoDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "\n",
        "    article = df['article'][i]\n",
        "\n",
        "    print(f\"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ ‚Ññ{i+1} ---\")\n",
        "\n",
        "    if pd.isna(article) or not str(article).strip():\n",
        "        print(\"! –°—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞—è –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.\")\n",
        "        print(\"-\" * 80)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏\n",
        "        summary = summarizer(\n",
        "            article[:1024],\n",
        "            max_length=130,\n",
        "            min_length=30,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        print(\"\\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
        "        print(str(article)[:500])\n",
        "        print(\"\\n–°–∞–º–º–∞—Ä–∏:\")\n",
        "        print(summary[0]['summary_text'])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}\")\n",
        "\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "AKn9d19KJydP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ó–∞–¥–∞—á–∞ 2: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
        "\n",
        "–¢–µ–ø–µ—Ä—å —Ä–µ—à–∏–º –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ –∏–º–µ–Ω–Ω–æ, –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π:\n",
        "- —Å–æ–∑–¥–∞–π—Ç–µ pipeline –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (\"sentiment-analysis\"), —É–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "- –≤–æ–∑—å–º–∏—Ç–µ –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ç–µ–∫—Å—Ç —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏)\n",
        "- –ø—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏, –≤—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n"
      ],
      "metadata": {
        "id": "WAdQe4OMxIQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        ")"
      ],
      "metadata": {
        "id": "Em5baHIixYwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['article'][0]"
      ],
      "metadata": {
        "id": "GeaS_woZ3HER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = sentiment_pipeline(df['article'][0])\n",
        "print(sentiment[0]['label'])\n",
        "print(sentiment[0]['score'])"
      ],
      "metadata": {
        "id": "T3LV9pus3OG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ–ø–µ—Ä—å –≤ —Ü–∏–∫–ª–µ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫ –ø–µ—Ä–≤—ã–º 10 –Ω–æ–≤–æ—Å—Ç—è–º. –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ?"
      ],
      "metadata": {
        "id": "x4--FaxVzE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "for i in range(10):\n",
        "    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏\n",
        "    article = df['article'][i]\n",
        "\n",
        "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    sentiment = sentiment_pipeline(article)\n",
        "\n",
        "    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    print(f\"\\n–ù–æ–≤–æ—Å—Ç—å ‚Ññ{i+1}:\")\n",
        "    print(\"\\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
        "    print(article[:500])\n",
        "    print(\"\\n–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:\")\n",
        "    print(sentiment[0]['label'])\n",
        "    print(\"\\n–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å:\")\n",
        "    print(sentiment[0]['score'])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "Ph75Ky5BzI_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å–Ω–∞—á–∞–ª–∞ —Å–¥–µ–ª–∞–π—Ç–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –ü—Ä–æ–¥–µ–ª–∞–π—Ç–µ —ç—Ç–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –≤ —Ü–∏–∫–ª–µ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 –Ω–æ–≤–æ—Å—Ç–µ–π. –¢–µ–ø–µ—Ä—å –≤—Å–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å?"
      ],
      "metadata": {
        "id": "ELRYX12DzK0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "xQjtcU797vDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        ")\n",
        "\n",
        "MAX_INPUT_CHARS = 4096\n",
        "\n",
        "for i in range(10):\n",
        "    article = df['article'][i]\n",
        "    print(f\"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ ‚Ññ{i+1} ---\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        article_text = str(article).strip()\n",
        "        safe_article_chunk = article_text[:MAX_INPUT_CHARS]\n",
        "        # –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–±—Ä–µ–∂–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –Ω–æ –ª—É—á—à–µ –ø–æ–¥–∞–≤–∞—Ç—å –µ–π —Ä–∞–∑—É–º–Ω—ã–π –∫—É—Å–æ–∫\n",
        "        summary_result = summarizer(\n",
        "            safe_article_chunk,\n",
        "            max_length=150,\n",
        "            min_length=40,\n",
        "            do_sample=False\n",
        "        )\n",
        "        summary_text = summary_result[0]['summary_text']\n",
        "\n",
        "\n",
        "        sentiment = sentiment_pipeline(summary_text)\n",
        "\n",
        "\n",
        "        print(\"\\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
        "        print(f\"{str(article)[:500]}...\")\n",
        "\n",
        "        print(\"\\n –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–°–∞–º–º–∞—Ä–∏):\")\n",
        "        print(summary_text)\n",
        "\n",
        "        print(\"\\n –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–ø–æ —Å–∞–º–º–∞—Ä–∏):\")\n",
        "        label = sentiment[0]['label']\n",
        "        score = sentiment[0]['score']\n",
        "\n",
        "\n",
        "        print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {label}\")\n",
        "        print(f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {score:.2%}\") # –í—ã–≤–æ–¥–∏–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"! –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}\")\n",
        "\n",
        "    finally:\n",
        "        print(\"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "moyOM0UhzT-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ó–∞–¥–∞—á–∞ 3: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)\n",
        "\n",
        "–ù–∞–∫–æ–Ω–µ—Ü, —Ä–µ—à–∏–º –∑–∞–¥–∞—á—É –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ–∫—Å—Ç–∞—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (Named Entity Recognition, NER). –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π:\n",
        "\n",
        "- —Å–æ–∑–¥–∞–π—Ç–µ pipeline –¥–ª—è NER (\"ner\"), —É–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ \"dslim/bert-base-NER\". –¢–∞–∫–∂–µ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ —É–∫–∞–∂–∏—Ç–µ aggregation_strategy=\"simple\" (—ç—Ç–æ —Å–ø–æ—Å–æ–± –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤)\n",
        "- –≤–æ–∑—å–º–∏—Ç–µ –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ç–µ–∫—Å—Ç —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏)\n",
        "- –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –º–æ–¥–µ–ª—å –∫ —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏, –≤—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
      ],
      "metadata": {
        "id": "svM2JCnxxwBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"dslim/bert-base-NER\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")"
      ],
      "metadata": {
        "id": "HNkPUxEy5xIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['article'][0]"
      ],
      "metadata": {
        "id": "ZcbE3uGuSu45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = ner_pipeline(df['article'][0])\n",
        "for entity in ner:\n",
        "        print(\n",
        "            f\"–°—É—â–Ω–æ—Å—Ç—å: {entity['word']}, \"\n",
        "            f\"–¢–∏–ø: {entity['entity_group']}, \"\n",
        "            f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {entity['score']:.2%}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "4FoQ2lyCS2fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—Ä–∏–º–µ–Ω–∏—Ç–µ –º–æ–¥–µ–ª—å –∫ –ø–µ—Ä–≤—ã–º 10 –Ω–æ–≤–æ—Å—Ç—è–º –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞. –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –ø—Ä–æ–±–ª–µ–º—ã - –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Ö –∏—Å–ø—Ä–∞–≤–∏—Ç—å."
      ],
      "metadata": {
        "id": "wX6mDkkp0CFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "\n",
        "for i in range(10):\n",
        "    try:\n",
        "        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏\n",
        "        article = df['article'][i]\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        if pd.isna(article):\n",
        "            print(f\"\\n–ù–æ–≤–æ—Å—Ç—å ‚Ññ{i+1}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç\")\n",
        "            continue\n",
        "\n",
        "        # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π\n",
        "        safe_article = str(article)[:4096].strip()\n",
        "\n",
        "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
        "        entities = ner_pipeline(safe_article)\n",
        "\n",
        "        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        print(f\"\\n--- –ù–æ–≤–æ—Å—Ç—å ‚Ññ{i+1} ---\")\n",
        "        print(f\"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞:\\n{str(article)[:500]}...\")\n",
        "        print(\"\\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:\")\n",
        "\n",
        "        if not entities:\n",
        "            print(\"–°—É—â–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "            continue\n",
        "\n",
        "        for entity in entities:\n",
        "            print(\n",
        "                f\"  –°—É—â–Ω–æ—Å—Ç—å: {entity['word']}, \"\n",
        "                f\"–¢–∏–ø: {entity['entity_group']}, \"\n",
        "                f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {entity['score']:.2%}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Ññ{i+1}: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "k49hUXxzz8B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ß–∞—Å—Ç—å 2: –ø—Ä–∏–º–µ–Ω—è–µ–º –±–æ–ª—å—à–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–ü–æ—Ä–∞–±–æ—Ç–∞–µ–º —Å –º–æ–¥–µ–ª—å—é [OpenChat](https://huggingface.co/openchat/openchat-3.5-0106).\n",
        "\n",
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –Ω–∏–∂–µ)."
      ],
      "metadata": {
        "id": "cBxOeHgO0LT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mh01-0_y0d9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = 'openchat/openchat-3.5-0106'\n",
        "\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        ")"
      ],
      "metadata": {
        "id": "tFqFfzuY0eWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –°–∫–æ–ª—å–∫–æ –≥–∏–≥–∞–±–∞–π—Ç GPU –∑–∞–Ω—è–ª–∞ –º–æ–¥–µ–ª—å?"
      ],
      "metadata": {
        "id": "xxrBrY3ysltK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–µ—Å—å\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –ø–∞–º—è—Ç–∏\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    return total_params\n",
        "\n",
        "def get_model_size_gb(model):\n",
        "    total_params = count_parameters(model)\n",
        "    total_bytes = total_params * 2\n",
        "    return total_bytes / (1024 ** 3)  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –≥–∏–≥–∞–±–∞–π—Ç—ã\n",
        "\n",
        "# –ü–æ–¥—Å—á—ë—Ç –ø–∞–º—è—Ç–∏\n",
        "model_size_gb = get_model_size_gb(model)\n",
        "print(f\"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ GPU: {model_size_gb:.2f} –ì–ë\")\n",
        "\n",
        "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU\n",
        "print(\"\\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU:\")\n",
        "print(f\"–û–±—â–∞—è –ø–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} –ì–ë\")\n",
        "print(f\"–ó–∞–Ω—è—Ç–æ –ø–∞–º—è—Ç–∏ GPU: {torch.cuda.memory_allocated() / (1024**3):.2f} –ì–ë\")\n",
        "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –ø–∞–º—è—Ç–∏ GPU: {torch.cuda.max_memory_allocated() / (1024**3):.2f} –ì–ë\")"
      ],
      "metadata": {
        "id": "3N8pjazKsrni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ: —Å –ø–æ–º–æ—â—å—é OpenChat –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–µ—à–∏—Ç—å –≤—Å–µ —Ç—Ä–∏ –∑–∞–¥–∞—á–∏ –≤—ã—à–µ.\n",
        "\n",
        "–†–µ—à–∏–º –ø–µ—Ä–≤—É—é –∑–∞–¥–∞—á—É (–∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏) –∑–∞ –≤–∞—Å. –ú—ã –ø–æ–ø—ã—Ç–∞–ª–∏—Å—å –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "–û—á–µ–Ω—å –≤–∞–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Å—Ä–µ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ google colab - —ç—Ç–æ GPU! –ò–Ω–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –∏–¥—Ç–∏ –æ–æ–æ–æ–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ."
      ],
      "metadata": {
        "id": "Gl0ftqk20uJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Classify the following article as positive, neutral or negative: \" + df['article'][0]\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "print(\"Input batch (encoded):\", batch)"
      ],
      "metadata": {
        "id": "m-aBwGIK0mBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(\n",
        "    **batch,\n",
        "    max_new_tokens=50, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞\n",
        "    temperature=0.7, # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "    top_p=0.9, # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_tokens = output_tokens[0][batch[\"input_ids\"].shape[1]:] # –≤—ã–≤–æ–¥–∏–º –Ω–∞ —ç–∫—Ä–∞–Ω —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ\n",
        "response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"üìÑ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "print(response.strip())"
      ],
      "metadata": {
        "id": "zNdau1lCy-gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü–æ–ª—É—á–∏–ª–æ—Å—å –Ω–µ –æ—á–µ–Ω—å :( –ö–∞–∫ –±—É–¥—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–Ω—è–ª–∞, —á–µ–≥–æ –º—ã –æ—Ç –Ω–µ–µ —Ö–æ—Ç–∏–º.\n",
        "\n",
        "–ú—ã –º–æ–∂–µ–º –¥–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤ –≤ —É–¥–æ–±–Ω–æ–º –¥–ª—è –Ω–µ–µ —à–∞–±–ª–æ–Ω–µ. –ü–æ–ø—Ä–æ–±—É–µ–º!"
      ],
      "metadata": {
        "id": "AK9Mm3t-zADF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "GPT4 Correct Assistant:\n",
        "You are a helpful assistant that classifies the sentiment of news headlines as Positive, Neutral, or Negative.\n",
        "Classify the sentiment of each news item as Positive, Neutral, or Negative.\n",
        "\n",
        "GPT4 Correct User:\n",
        "Question: Apple's new iPhone sales break records in first weekend.\n",
        "GPT4 Correct Assistant:\n",
        "Correct Answer: POSITIVE <|end_of_turn|>\n",
        "\n",
        "GPT4 Correct User:\n",
        "Question: The city council met Tuesday to discuss zoning regulations.\n",
        "GPT4 Correct Assistant:\n",
        "Correct Answer: NEUTRAL <|end_of_turn|>\n",
        "\n",
        "GPT4 Correct User:\n",
        "Question: Severe floods displace thousands in southern regions.\n",
        "GPT4 Correct Assistant:\n",
        "Correct Answer: NEGATIVE <|end_of_turn|>\n",
        "\n",
        "GPT4 Correct User: Question: \"\"\" + df['article'][0] + \"\"\"\n",
        "GPT4 Correct Assistant:\n",
        "\"\"\".strip()\n",
        "\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "print(\"Input batch (encoded):\", batch)"
      ],
      "metadata": {
        "id": "lUZjr02qwwPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(\n",
        "    **batch,\n",
        "    max_new_tokens=50, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞\n",
        "    temperature=0.7, # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "    top_p=0.9, # –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_tokens = output_tokens[0][batch[\"input_ids\"].shape[1]:] # –≤—ã–≤–æ–¥–∏–º –Ω–∞ —ç–∫—Ä–∞–Ω —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ\n",
        "response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"üìÑ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "print(response.strip())"
      ],
      "metadata": {
        "id": "klMD08OS1EH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í–∞—à–∞ –æ—á–µ—Ä–µ–¥—å!\n",
        "\n",
        "- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–º–ø—Ç (—Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ NER –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "\n",
        "–û—Ü–µ–Ω–∏—Ç–µ –≤–∏–∑—É–∞–ª—å–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–º –≤—ã–≤–æ–¥—ã:\n",
        "- –•–æ—Ä–æ—à–æ –ª–∏ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å LLM?\n",
        "- LLM —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å –ª—É—á—à–µ, —á–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ —á–∞—Å—Ç–∏ 1 –∏–ª–∏ —Ö—É–∂–µ?"
      ],
      "metadata": {
        "id": "Mo3psB8G1Jb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "# –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏\n",
        "summarization_prompt = f\"\"\"\n",
        "GPT4 Summarizer:\n",
        "You are an expert news summarizer. Create a concise summary of the news article in 3-4 sentences, preserving the main ideas and key facts.\n",
        "\n",
        "Article: {df['article'][0]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "batch = tokenizer(summarization_prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n"
      ],
      "metadata": {
        "id": "4wfjgTH91jfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(\n",
        "    **batch,\n",
        "    max_new_tokens=150,  # —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_tokens = output_tokens[0][batch[\"input_ids\"].shape[1]:]\n",
        "summary = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nüìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è:\")\n",
        "print(summary.strip())"
      ],
      "metadata": {
        "id": "mquzltSyhtlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü—Ä–æ–º–ø—Ç –¥–ª—è NER\n",
        "ner_prompt = f\"\"\"\n",
        "Named Entity Recognizer:\n",
        "Extract all named entities from the text and classify them into categories:\n",
        "- PERSON (–ª—é–¥–∏)\n",
        "- ORG (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)\n",
        "- LOC (–º–µ—Å—Ç–∞)\n",
        "- DATE (–¥–∞—Ç—ã)\n",
        "- MONEY (–¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã)\n",
        "- EVENT (—Å–æ–±—ã—Ç–∏—è)\n",
        "\n",
        "Format your response as a list of entities with their types:\n",
        "\n",
        "Text: {df['article'][0]}\n",
        "\n",
        "Entities:\n",
        "\"\"\"\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "batch = tokenizer(ner_prompt, return_tensors='pt', return_token_type_ids=False).to(device)"
      ],
      "metadata": {
        "id": "q4QrLLtSiSWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(\n",
        "    **batch,\n",
        "    max_new_tokens=200,  # —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_tokens = output_tokens[0][batch[\"input_ids\"].shape[1]:]\n",
        "entities = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏:\")\n",
        "print(entities.strip())"
      ],
      "metadata": {
        "id": "CXtjRBxAiXUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –í—ã–≤–æ–¥—ã\n",
        "\n",
        "LLM —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å —Å –∑–∞–¥–∞—á–µ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, –Ω–æ –≤ –æ—Ç–ª–∏—á–∏–∏ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ–±–∞–≤–∏–ª–∞ –≤—Å—ë-—Ç–∞–∫–∏ –ª–∏—à–Ω—é—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –û–Ω–∞ —Ö—É–∂–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–∞—Ö, –¥–æ–±–∞–≤–ª—è—è –ª–∏—à–Ω–∏–µ –¥–µ—Ç–∞–ª–∏\n",
        "\n",
        "–ë–∞–∑–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞ —Å—á–µ—Ç –º–æ–¥–µ–ª–µ–π —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å –≤—Å—ë-—Ç–∞–∫–∏ –ª—É—á—à–µ, —á–µ–º LLM.\n",
        "\n",
        "–° –∑–∞–¥–∞—á–µ–π NER LLM —Ç–∞–∫–∂–µ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å —Ö—É–∂–µ –º–æ–¥–µ–ª–µ–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∞ –Ω–∞—à–ª–∞ –º–µ–Ω—å—à–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –û—à–∏–±–∫–∏ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –Ω–∞—à–ª–∏—Å—å –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö. –ú–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ LLM –Ω–∞—à–ª–∞ —Ç–µ —Å—É—â–Ω–æ—Å—Ç–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —É —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ–∫–æ–ª–æ 99%. –ê —Å—É—â–Ω–æ—Å—Ç–∏ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, 90% LLM –Ω–∞–π—Ç–∏ —É–∂–µ –Ω–µ —Å–º–æ–≥–ª–∞"
      ],
      "metadata": {
        "id": "Fq2uSKbkTGKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ß–∞—Å—Ç—å 3: —Ç–≤–æ—Ä—á–µ—Å–∫–∞—è\n",
        "\n",
        "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ [—Å–æ–±—Ä–∞—Ç—å RAG-—Å–∏—Å—Ç–µ–º—É](https://colab.research.google.com/drive/196TKVfLWesbrF7f4KNHMGZNJC3hfg5IK?usp=sharing), –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —ç—Ç–æ –Ω–∞ –≤–µ–±–∏–Ω–∞—Ä–µ, –¥–ª—è –º–æ–¥–µ–ª–∏ OpenChat (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–¥ –ø–æ —Å—Å—ã–ª–∫–µ, —Ç–æ–ª—å–∫–æ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–º–æ—Ç—Ä–∏—Ç–µ, –≤—Å–µ –ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã –∏–º–µ–Ω–Ω–æ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏).\n",
        "\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö –¥–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º [rag-mini-bioasq](https://huggingface.co/datasets/enelpol/rag-mini-bioasq), —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ —Å—Ö–æ–∂—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Ç–µ–º, —á—Ç–æ –±—ã–ª –≤ —Å–µ–º–∏–Ω–∞—Ä–µ, –Ω–æ —Ç–µ–ø–µ—Ä—å –æ–Ω –ø–æ –±–∏–æ-–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ. –≠—Ç–æ —Å–∞–±–¥–∞—Ç–∞—Å–µ—Ç —Å –≤–æ–∫—Ä—à–æ–ø–∞ [the BioASQ Challenge](https://www.bioasq.org/), —Ç–æ –µ—Å—Ç—å –≤—ã —Ä–µ—à–∞–µ—Ç–µ –∑–∞–¥–∞—á—É —Å–∞–º–æ–≥–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –≤ —Å—Ñ–µ—Ä–µ –±–∏–æ-–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ NLP!\n",
        "\n",
        "–û—Å–Ω–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å, —Å –∫–æ—Ç–æ—Ä–æ–π –≤—ã —Å—Ç–∞–ª–∫–Ω–µ—Ç–µ—Å—å, –∫–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å –ø–æ–ª—É—á–∏–≤—à—É—é—Å—è RAG-—Å–∏—Å—Ç–µ–º—É. –ê–≤—Ç–æ—Ä—ã —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –¥–µ–ª–∞—é—Ç —ç—Ç–æ [—Ç–∞–∫](http://participants-area.bioasq.org/Tasks/b/eval_meas_2022/): –æ–Ω–∏ —Ä–∞–∑–±–∏–≤–∞—é—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏ - –æ—Ü–µ–Ω–∫–∞ retrievel –∏ –æ—Ü–µ–Ω–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –î–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º —Ç–∞–∫ –∂–µ, –Ω–æ —É–ø—Ä–æ—Å—Ç–∏–º –Ω–∞—à—É —Å–∏—Å—Ç–µ–º—É.\n",
        "\n",
        "–û–±–∞ —Å–ø–æ—Å–æ–±–∞ –æ—Ü–µ–Ω–∫–∏ –≤—ã —Ä–µ–∞–ª–∏–∑—É–µ—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –î–ª—è —á–∞—Å—Ç–∏ —Å answer –¥–∞–≤–∞–π—Ç–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å—ã —Ñ–æ—Ä–º–∞—Ç–∞ yes/no –∏ –ø—Ä–æ—Å—Ç–æ –ø–æ—Å—á–∏—Ç–∞–µ–º F1 –ø–æ –¥–≤—É–º –∫–ª–∞—Å—Å–∞–º. –ê –¥–ª—è —á–∞—Å—Ç–∏ —Å retrievel –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç—Ä–∏–∫—É Hit Rate@k - –ø–æ–ø–∞–ª –ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ø–µ—Ä–≤—ã–µ *k*? –§–æ—Ä–º–∞ –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–∞—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ *i*:\n",
        "\n",
        "Hit Rate_i\\@k\n",
        "\n",
        "  $$\n",
        "    \\text{success}_i =\n",
        "      \\begin{cases}\n",
        "        1, & \\text{–µ—Å–ª–∏ } |\\,\\text{gold}_i \\cap \\text{retrieved}_i^{[:k]}| > 0 \\\\\n",
        "        0, & \\text{–∏–Ω–∞—á–µ}\n",
        "      \\end{cases}\n",
        "  $$\n",
        "\n",
        "–ó–∞—Ç–µ–º –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –≤—Å–µ–º *N* –≤–æ–ø—Ä–æ—Å–∞–º:\n",
        "\n",
        "$$\n",
        "  \\text{Hit Rate@k} = \\frac{1}{N}\\sum_{i=1}^{N}\\text{success}_i.\n",
        "$$\n",
        "\n",
        "–î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –º—ã –ø—Ä–æ—Å—Ç–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å—á–∏—Ç–∞–µ–º, –ø–æ–ø–∞–ª –ª–∏ –æ–Ω –≤ —Ç–æ–ø_k –∏–ª–∏ –Ω–µ—Ç, –∞ –ø–æ—Ç–æ–º —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Å–µ–º –≤–æ–ø—Ä–æ—Å–∞–º. –î–∞–≤–∞–π—Ç–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ k –≤–æ–∑—å–º–µ–º 3, —Ç–æ –µ—Å—Ç—å –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å Hit Rate@3.\n",
        "\n",
        "*–ü—Ä–∏–º–µ—Ä*\n",
        "\n",
        "| –í–æ–ø—Ä–æ—Å | –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫–æ–≤ (gold) | –ß—Ç–æ –≤–µ—Ä–Ω—É–ª–∞ —Å–∏—Å—Ç–µ–º–∞ (—Ç–æ–ø-3) | Hit Rate\\@3   \n",
        "| ------ | ------------------------ | --------------------------- | ------------\n",
        "| Q1     | {A, B}                   | A, C, D                     | 1 (A –Ω–∞–π–¥–µ–Ω)\n",
        "| Q2     | {E}                      | F, G, H                     | 0            \n",
        "| Q3     | {J, K, L}                | K, L, A                     | 1 (A –Ω–∞–π–¥–µ–Ω)      \n",
        "\n",
        "–°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: *Hit Rate\\@3* = (1 + 0 + 1)/3 = 0.67\n",
        "\n",
        "–û—Å—Ç–∞–ª–æ—Å—å –ø–æ–Ω—è—Ç—å, –≥–¥–µ –≤–∑—è—Ç—å –∑–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç –∏ —á—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏. 1) –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å relevant_passage_ids - —ç—Ç–æ –∏ –µ—Å—Ç—å –∑–æ–ª–æ—Ç—ã–µ (golden) –æ—Ç–≤–µ—Ç—ã. 2) –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, –∫ –∫–∞–∫–æ–≥–æ relevant_passage_ids –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–æ—Ç —Ç–∞–∫: metadata={\"id\": rec[\"id\"], \"chunk\": i}.\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã –≤–¥—Ä—É–≥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞–Ω–∏—è —Å—Ç–æ–ª–∫–Ω–µ—Ç–µ—Å—å —Å–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—è–º–∏, –Ω–µ –±–æ–π—Ç–µ—Å—å –ø–æ–¥–≥–ª—è–¥—ã–≤–∞—Ç—å –≤ —Ä–µ—à–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –¥–∞—Ç–∞—Å–∞–µ–Ω—Ç–∏—Å—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, [kaggle](https://www.kaggle.com/) - —ç—Ç–æ –∫–ª–∞–¥–µ–∑—å –æ—Ç–ª–∏—á–Ω—ã—Ö –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, –∏ RAG –≤ —Ç–æ–º —á–∏—Å–ª–µ. –ú–æ–∂–Ω–æ, –¥–æ–ø—É—Å—Ç–∏–º, –∑–∞–≥–ª—è–Ω—É—Ç—å –≤ [–Ω–æ—É—Ç–±—É–∫](https://www.kaggle.com/code/erwanversmee/rag-for-mini-bioasq/notebook) –∫ Erwan Versmee (–µ—Å–ª–∏ –Ω–∞–π–¥–µ—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–ª–µ–∑–Ω—ã–º, –ø–æ—Å—Ç–∞–≤—å—Ç–µ –ª–∞–π–∫ - —á–µ–ª–æ–≤–µ–∫—É –±—É–¥–µ—Ç –ø—Ä–∏—è—Ç–Ω–æ). –ö–∞–∂–¥—ã–π –∫–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª–µ–Ω, –∏ —á–µ–º –±–æ–ª—å—à–µ –≤—ã —É–≤–∏–¥–∏—Ç–µ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ —Å—Ç–∞–Ω–µ—Ç–µ –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç. –£–¥–∞—á–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞–Ω–∏—è!"
      ],
      "metadata": {
        "id": "2zDfkf2z1p8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pips"
      ],
      "metadata": {
        "id": "Q0sK0X64lq5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –≤–∞—à –∫–æ–¥ –∑–¥–µ—Å—å\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "!pip uninstall -y fastai torch torchvision torchaudio gcsfs fsspec"
      ],
      "metadata": {
        "id": "w16WFY1B2BUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers torch bitsandbytes"
      ],
      "metadata": {
        "id": "lBwKS_9pCH98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 \\\n",
        "    --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "qUpFqcywlhQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fsspec==2025.3.2 gcsfs==2025.3.2"
      ],
      "metadata": {
        "id": "F50kcJgNliso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "    bitsandbytes==0.46.0 \\\n",
        "    transformers==4.52.4 \\\n",
        "    accelerate==1.8.1 \\\n",
        "    datasets==3.6.0 \\\n",
        "    sentence-transformers==4.1.0 \\\n",
        "    chromadb==1.0.13 \\\n",
        "    langchain==0.3.26 \\\n",
        "    langchain-community==0.3.26 \\\n",
        "    langchain-huggingface \\\n",
        "    tqdm==4.67.1"
      ],
      "metadata": {
        "id": "NaDWSdC4ljse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers bitsandbytes\n",
        "!pip install transformers==4.32.0 bitsandbytes\n",
        "!pip install langchain_community chromadb sentence-transformers"
      ],
      "metadata": {
        "id": "spAaDGWbprVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BitsAndBytes"
      ],
      "metadata": {
        "id": "b7X6YcnBltTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch, transformers\n",
        "import torchvision\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "TIw8NUrjARwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch import cuda, bfloat16\n",
        "# import torch, transformers\n",
        "# import torchvision\n",
        "# from torch.optim.lr_scheduler import LRScheduler\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "DEVICE = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "be-mHclZlvVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(transformers.__version__, sys.executable)"
      ],
      "metadata": {
        "id": "p7WhK68blxZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb, importlib, os\n",
        "print(bnb.__version__)\n",
        "!ldconfig -p | grep cusparse | head -n 3"
      ],
      "metadata": {
        "id": "yYGh4Y1SlykB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "DkVBZH42l-qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'openchat/openchat-3.5-0106'\n",
        "\n",
        "# –ö–≤–∞–Ω—Ç—É–µ–º –≤¬†4¬†–±–∏—Ç–∞, —á—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏–ª–æ—Å—å –≤¬†VRAM 6‚Äì8¬†–ì–ë\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                          # –≤–∫–ª—é—á–∏—Ç—å 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n",
        "    bnb_4bit_quant_type=\"nf4\",                  # —Ç–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: \"nf4\" (Normalized Float 4) –∏–ª–∏ \"fp4\"\n",
        "    bnb_4bit_use_double_quant=True,             # –≤–∫–ª—é—á–∏—Ç—å –¥–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è)\n",
        "    bnb_4bit_compute_dtype=torch.float16        # —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, bfloat16 (–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ T4), float16)\n",
        ")\n",
        "\n",
        "\n",
        "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ‚Ä¶\")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    # trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
      ],
      "metadata": {
        "id": "Y_5uR-aPl_-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –î–∞–Ω–Ω—ã–µ"
      ],
      "metadata": {
        "id": "A4XoxnqToqLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from langchain.document_loaders import TextLoader  # –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –≤ –æ–±—ä–µ–∫—Ç—ã Document –¥–ª—è LangChain.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (chunks).\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞.\n",
        "from langchain.vectorstores import Chroma  # –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Chroma: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –∏—â–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HuggingFace Transformers pipeline –∫–∞–∫ LLM-–º–æ–¥—É–ª—å –≤ LangChain.\n",
        "from langchain.chains import RetrievalQA  # –≥–æ—Ç–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ ¬´–ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞¬ª (Retrieval-augmented QA)."
      ],
      "metadata": {
        "id": "Ke5a1yeYoT4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", name=\"text-corpus\", split=\"passages\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "corpus_docs = [\n",
        "    Document(page_content=rec[\"passage\"], metadata={\"id\": rec[\"id\"]})\n",
        "    for rec in bio_ds\n",
        "]\n",
        "\n",
        "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(corpus_docs)}\")\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:\\n\", corpus_docs[0].page_content[:200], \"...\")"
      ],
      "metadata": {
        "id": "p_jnFZpAosER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_ds"
      ],
      "metadata": {
        "id": "8yGB5MXlrJRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512, chunk_overlap=50\n",
        ")\n",
        "\n",
        "# docs = splitter.split_documents(corpus_docs)\n",
        "# print(\"–ß–∞–Ω–∫–æ–≤:\", len(docs))\n",
        "\n",
        "docs = []\n",
        "for doc in corpus_docs:\n",
        "    chunks = splitter.split_documents([doc])\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk.metadata[\"chunk\"] = i\n",
        "        docs.append(chunk)\n",
        "\n",
        "print(\"–ß–∞–Ω–∫–æ–≤:\", len(docs))"
      ],
      "metadata": {
        "id": "dQL01yWhrQPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã"
      ],
      "metadata": {
        "id": "RzPeIhm4rvic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"chroma_ragmini\"\n",
        "if os.path.exists(persist_directory):\n",
        "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "else:\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "        model_kwargs={\"device\": \"cuda\"}\n",
        "    )\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    vectordb.persist()"
      ],
      "metadata": {
        "id": "7K4EM6qLrw05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–±–æ—Ä–∫–∞ —Ü–µ–ø–æ—á–∫–∏"
      ],
      "metadata": {
        "id": "xP7n2QomtgSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "if3XO3u02ss0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def evaluate_retrieval(retriever, val_ds, k=3):\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "    val_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\",\n",
        "                         name=\"question-answer-passages\",\n",
        "                         split=\"test\").select(range(10))\n",
        "\n",
        "    hit_rates = []\n",
        "    results = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "\n",
        "    for example in val_ds:\n",
        "        question = example[\"question\"]\n",
        "\n",
        "        try:\n",
        "            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª\n",
        "            relevant_ids_str = example[\"relevant_passage_ids\"]\n",
        "            gold_ids = set(ast.literal_eval(relevant_ids_str))\n",
        "\n",
        "            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "            docs = retriever.get_relevant_documents(question)[:k]\n",
        "            retrieved_ids = set(doc.metadata[\"id\"] for doc in docs)\n",
        "\n",
        "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å –∑–æ–ª–æ—Ç—ã–º–∏ id\n",
        "            hit = 1 if gold_ids.intersection(retrieved_ids) else 0\n",
        "            hit_rates.append(hit)\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"gold_ids\": gold_ids,\n",
        "                \"retrieved_ids\": retrieved_ids,\n",
        "                \"hit\": hit\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ '{question}': {e}\")\n",
        "            hit_rates.append(0)\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
        "    print(\"\\n–ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
        "    for res in results:\n",
        "        print(f\"\\n–í–æ–ø—Ä–æ—Å: {res['question']}\")\n",
        "        print(f\"–ó–æ–ª–æ—Ç—ã–µ ID: {res['gold_ids']}\")\n",
        "        print(f\"–ù–∞–π–¥–µ–Ω–Ω—ã–µ ID: {res['retrieved_ids']}\")\n",
        "        print(f\"Hit: {'–î–∞' if res.get('hit', 0) else '–ù–µ—Ç'}\")\n",
        "\n",
        "    return np.mean(hit_rates) if hit_rates else 0.0"
      ],
      "metadata": {
        "id": "g3PrvgEM4f0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_answers(qa_chain, val_ds):\n",
        "    val_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\",\n",
        "                         name=\"question-answer-passages\",\n",
        "                         split=\"test\").select(range(10))\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    results = []\n",
        "\n",
        "    print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\")\n",
        "\n",
        "    for example in val_ds:\n",
        "        question = example[\"question\"]\n",
        "        gold_answer = example[\"answer\"].strip().lower()\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞\n",
        "        is_yes_no = gold_answer in [\"yes\", \"no\"]\n",
        "\n",
        "        try:\n",
        "            response = qa_chain.invoke({\"query\": question})[\"result\"].strip().lower()\n",
        "\n",
        "            if is_yes_no:\n",
        "                true_label = 1 if gold_answer == \"yes\" else 0\n",
        "                predicted = 1 if response == \"yes\" else 0\n",
        "\n",
        "                true_labels.append(true_label)\n",
        "                predicted_labels.append(predicted)\n",
        "\n",
        "                results.append({\n",
        "                    \"question\": question,\n",
        "                    \"gold_answer\": gold_answer,\n",
        "                    \"model_response\": response,\n",
        "                    \"correct\": true_label == predicted\n",
        "                })\n",
        "\n",
        "                print(f\"\\n–í–æ–ø—Ä–æ—Å: {question}\")\n",
        "                print(f\"–ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold_answer}\")\n",
        "                print(f\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response}\")\n",
        "                print(f\"–í–µ—Ä–Ω–æ: {'–î–∞' if true_label == predicted else '–ù–µ—Ç'}\")\n",
        "\n",
        "            else:\n",
        "                # print(f\"\\n–í–æ–ø—Ä–æ—Å: {question}\")\n",
        "                # print(f\"–ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold_answer}\")\n",
        "                # print(f\"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response}\")\n",
        "                print(\"–≠—Ç–æ –Ω–µ yes/no –≤–æ–ø—Ä–æ—Å, –æ—Ü–µ–Ω–∫–∞ –Ω–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ '{question}': {e}\")\n",
        "\n",
        "    if true_labels:\n",
        "        f1 = f1_score(true_labels, predicted_labels)\n",
        "        print(f\"\\nF1 score –¥–ª—è yes/no –≤–æ–ø—Ä–æ—Å–æ–≤: {f1:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n–ù–µ—Ç yes/no –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ\")\n",
        "\n",
        "    return f1"
      ],
      "metadata": {
        "id": "JomW5q6J4iFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "TTR5FYg2J7hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig\n",
        "from langchain import PromptTemplate\n"
      ],
      "metadata": {
        "id": "epkbLdFFyRVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_cfg = GenerationConfig(\n",
        "    max_length=2048,\n",
        "    max_new_tokens=128,\n",
        "    # temperature=0.1,\n",
        "    # do_sample=False,\n",
        "    # top_p=0.9\n",
        ")"
      ],
      "metadata": {
        "id": "HFaJ4596u-Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    ),\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 128,\n",
        "        \"temperature\": 0.1,\n",
        "        \"do_sample\": False\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "9X6in405vGM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_ds.column_names)"
      ],
      "metadata": {
        "id": "pCkq1vPvyzJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "# template = \"\"\"\n",
        "# Answer the question based on the provided context. Follow these rules strictly:\n",
        "\n",
        "# 1. For yes/no questions:\n",
        "#    - Respond ONLY with \"Yes\" or \"No\"\n",
        "#    - No additional explanations\n",
        "\n",
        "# 2. For open-ended questions:\n",
        "#    - Provide a concise answer (1-2 sentences)\n",
        "#    - Use only information from the context\n",
        "#    - Keep the answer relevant and to the point\n",
        "\n",
        "# 3. DO NOT include:\n",
        "#    - Any instructions\n",
        "#    - Role descriptions\n",
        "#    - Extra text\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question: {question}\n",
        "\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "# prompt = PromptTemplate(\n",
        "#     template=template,\n",
        "#     input_variables=[\"context\", \"question\"]\n",
        "# )\n"
      ],
      "metadata": {
        "id": "qhCM6sPrvGkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "template = \"\"\"\n",
        "Answer ONLY the question with a direct response.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ZrTCDlF5JqxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", name=\"question-answer-passages\", split=\"test\").select(range(10))\n",
        "\n",
        "# test_samples = val_ds.select(range(10))"
      ],
      "metadata": {
        "id": "gVqzYptyPAGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, example in enumerate(val_ds):\n",
        "        if i >= 10:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 10 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "            break\n",
        "\n",
        "        question = example[\"question\"]\n",
        "        answer = example[\"answer\"].lower()\n",
        "\n",
        "        print(f\"\\n–í–æ–ø—Ä–æ—Å {i+1}: {question}\")\n",
        "        print(f\"–û—Ç–≤–µ—Ç: {answer}\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–æ–ø—Ä–æ—Å yes/no\n",
        "        if answer in [\"yes\", \"no\"]:\n",
        "            print(\"–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: Yes/No\")\n",
        "        else:\n",
        "            print(\"–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: –û—Ç–∫—Ä—ã—Ç—ã–π\")"
      ],
      "metadata": {
        "id": "unPDo5yr9fLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "def evaluate_single_answer(chain, question, gold_answer):\n",
        "    try:\n",
        "        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏\n",
        "        full_response = chain.run(question)\n",
        "\n",
        "        print(f\"\\n–ü–û–õ–ù–´–ô –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò: {full_response}\")\n",
        "\n",
        "        # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
        "        cleaned_response = full_response.strip().lower().split()[0] if full_response else \"\"\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞\n",
        "        is_yes_no = gold_answer.lower() in [\"yes\", \"no\"]\n",
        "\n",
        "        if is_yes_no:\n",
        "            true_label = 1 if gold_answer.lower() == \"yes\" else 0\n",
        "            predicted = 1 if cleaned_response == \"yes\" else 0\n",
        "\n",
        "            print(f\"\\n–í–æ–ø—Ä–æ—Å: {question}\")\n",
        "            print(f\"–ó–æ–ª–æ—Ç–æ–π –æ—Ç–≤–µ—Ç: {gold_answer}\")\n",
        "            print(f\"–û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {cleaned_response}\")\n",
        "            print(f\"–í–µ—Ä–Ω–æ: {'–î–∞' if true_label == predicted else '–ù–µ—Ç'}\")\n",
        "\n",
        "            return true_label, predicted\n",
        "        else:\n",
        "            print(\"–≠—Ç–æ –Ω–µ yes/no –≤–æ–ø—Ä–æ—Å\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\n",
        "question = \"Is the protein Papilin secreted?\"\n",
        "gold_answer = \"Yes\"\n",
        "\n",
        "true_label, predicted = evaluate_single_answer(chain, question, gold_answer)"
      ],
      "metadata": {
        "id": "XJJ7qqhoIwmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hit_rate = evaluate_retrieval(retriever, val_ds, k=3)\n",
        "print(f\"Hit Rate@3: {hit_rate:.4f}\")\n",
        "\n",
        "answer_f1 = evaluate_answers(qa_chain, val_ds)\n",
        "print(f\"F1 score –¥–ª—è yes/no –≤–æ–ø—Ä–æ—Å–æ–≤: {answer_f1:.4f}\")"
      ],
      "metadata": {
        "id": "h9TuGEmu5ENb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "\n",
        "–ú–Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å –º–µ—Ç—Ä–∏–∫–∏ hit rate@k = 0.7\n",
        "\n",
        "ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–µ—Ä–Ω–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤–æ –≤—Å–µ—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö.\n",
        "\n",
        "–ù–æ, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –º–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫—É yes/no –æ—Ç–≤–µ—Ç–æ–≤ —Ç–∞–∫, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å —Å—á–∏—Ç—ã–≤–∞–ª–∞ –∏—Ö –∫–∞–∫ –ø–æ–¥–æ–±–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –û–Ω–∞ –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–ª–∞ –∫ –æ—Ç–≤–µ—Ç–∞–º –ª–∏—à–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ö–æ—Ç—å –∏ —Å–∞–º–∏ –æ—Ç–≤–µ—Ç—ã  yes/no –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —Å–æ–≤–ø–∞–¥–∞–ª–∏\n",
        "\n",
        "–ü–æ—ç—Ç–æ–º—É –æ—Ü–µ–Ω–∏—Ç—å f1-score –º–æ–∂–Ω–æ –±—ã–ª–æ —Ç–æ–ª—å–∫–æ –≤—Ä—É—á–Ω—É—é, —á—Ç–æ —è, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Ç–æ–∂–µ –∑–∞–±—ã–ª —Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ (–∞ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ gpu –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –æ—Å—Ç–∞–≤–∞–ª–æ—Å—å –Ω–µ–º–Ω–æ–≥–æ). –ù–æ –ø–æ –º–æ–µ–π –ø–∞–º—è—Ç–∏ –≤—Å–µ –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è –≤–∏–¥–µ–ª(5-10 —à—Ç—É–∫), —Å–æ–≤–ø–∞–¥–∞–ª–∏."
      ],
      "metadata": {
        "id": "1iX5X59cWVli"
      }
    }
  ]
}